---
title: "STA 5224: Final Project - Titanic Dataset"
author: "Huong Tran"
date: "3/27/2022"
output: pdf_document
---

# I. Project Proposal: 
## Objective:
This project is a competition on Kaggle with target of predicting which passengers survived the Titanic shipwreck by machine learning. Part 1 of the project will focus on cleaning data and obtaining some insights about data, together with variable selection process to create a meaningful dataset that can be used. Model application will be in the second part. Besides the statistical approach in logistic models, other machines learning model will be used. A typical model for supervised learning is decision tree, which is a series of sequential decisions represented as a tree to get a specific result. However, decision tree are prone to overfitting, especially when the tree is deep and random forest is a solution for this kind of problem. In general, a random forests model will creates several random decision trees and aggregate their result. Also, SVM works well with classification and regression, therefore it is good to represent SVM.  Finally, the comparsions between these models will be derived. In the last section, statistical inference and interpretation from logistic models will be discussed. 

## About the dataset:
The data is obtained from the Titanic competition from [Kaggle](https://www.kaggle.com/c/titanic). While the test.csv and gender_submission.csv will be used for model training, the train.csv will be used to evaluate model performance. 

The dependence variable is "Survived", which has value 0 or 1, indicates that the person survived after the disaster or not. The others are exploratory variables, with their meaning can be find at the website.

```{r}
test.org <- read.csv2(
  "/Users/huongtran/OU /Course Work/SES 4/STA5224/Final Project 2/data/test.csv", 
  header = T, sep = ",")

survive <- read.csv(
  "/Users/huongtran/OU /Course Work/SES 4/STA5224/Final Project 2/data/gender_submission.csv")

train.org <- read.csv2(
  "/Users/huongtran/OU /Course Work/SES 4/STA5224/Final Project 2/data/train.csv", 
  header = T, sep = ","
  )

summary(train.org)
```

```{r}
colnames(train.org)
nrow(train.org)
```
There are 891 observations and 10 features in this dataset, since PassengerId is unique, we can not extract any information from this variable and "Survived" is the dependent variable. 

# II. EDA (Exploratory Data Analysis):
```{r, message= F, warning= F}
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(gmodels)
library(vcd)
library(caret)
```


```{r}
anyDuplicated(train.org)
```
There is no duplicate rows in this dataset. 

```{r}
colSums(is.na(train.org))
```
At first, it seems that there is no missing value in this dataset, but in fact, there are some cells having value of empty string, and containing no information, those are considered as missing value. 

```{r}
empty.string <- c()
for (i in colnames(train.org)){
  empty.string <- c(empty.string, sum(train.org[,i] == ""))
}
names(empty.string) <- colnames(train.org)
empty.string
```
```{r}
empty.string.test <- c()
for (i in colnames(test.org)){
  empty.string.test <- c(empty.string.test, sum(test.org[,i] == ""))
}


names(empty.string.test) <- colnames(test.org)
empty.string.test
```


```{r}
cat("Number of missing value in Cabin in train file: " , empty.string["Cabin"], 
    "account for ", round(empty.string["Cabin"] / nrow(train.org) * 100, 2) ,
    "% in total observation", "\n ")

cat("Number of missing value in Cabin in test file: " , empty.string.test["Cabin"],
    "account for ",
    round( empty.string.test["Cabin"]/nrow(test.org) * 100, 2), 
    "% in total observation")
```

Cabin variables has  $77\%$ missing value in train set and  $78.23\%$ in test set, therefore I will drop this variable. 




## 1. What about name?
In the variable "name", there is also title of the person, which indicates their social class and profession.

```{r}
train["Title"] <- str_split_fixed(train$Name, ", ", n = 2)[,2]
train["Title"] <- str_split_fixed(train$Title, ". ", n = 2)[, 1]
unique(train$Title)
```

The titles relating to army and "Rev" are less likely to survive, we can just merge them in one level as "Official", all of observation in this variables are male. 
```{r}
train[which(train$Title %in% c("Major", "Capt", "Col", "Don", "Rev")), c("Survived", "Title", "Sex")]
```

```{r}
train["Title"] <- gsub("Major", "Officer", train[,"Title"], fixed = T)
train["Title"] <- gsub("Capt", "Officer", train[,"Title"], fixed = T)
train["Title"] <- gsub("Col", "Officer", train[,"Title"], fixed = T)
train["Title"] <- gsub("Rev", "Officer", train[,"Title"], fixed = T)
train["Title"] <- gsub("Don", "Officer", train[,"Title"], fixed = T)
unique(train$Title)
```


```{r}
train[which(train$Title %in% c("Mme", "Mlle", "th", "Jonkheer", "Lady", "Sir")),
      c("Title","Pclass", "Sex", "Age", "Name", "SibSp", "Survived")]
```
 
For the title listed above, they all represent for unmarried women, therefore, we should change them into "Miss. And "Jonkheer" will be changed into "Mr". Also the only one value of "Lady" and "Sir" are spouse, we will change their title into "Mrs" and "Mr". 
```{r}
train["Title"] <- gsub("Mme", "Miss", train[,"Title"], fixed = T)
train["Title"] <- gsub("Mlle", "Miss", train[,"Title"], fixed = T)
train["Title"] <- gsub("th", "Miss", train[,"Title"], fixed = T)
train["Title"] <- gsub("Jonkheer", "Mr", train[,"Title"], fixed = T)
train["Title"] <- gsub("Ms", "Miss", train[,"Title"], fixed = T)
train["Title"] <- gsub("Ms", "Miss", train[,"Title"], fixed = T)
train["Title"] <- gsub("Ms", "Mrs", train[,"Title"], fixed = T)
train["Title"] <- gsub("Lady", "Mrs", train[,"Title"], fixed = T)
train["Title"] <- gsub("Sir", "Mr", train[,"Title"], fixed = T)

unique(train$Title)
```


```{r}
df.title <- as.data.frame(table(train$Survived, train$Title))
names(df.title)
ggplot(df.title, aes(x = Var2, y = Freq, fill = Var1)) + geom_col(position = "fill") 
```

"Miss" and "Mrs" had the highest chance to survived, while "Official" and "Mr" are the two groups with lowest chance to survive.

Also, the information of sex is included in title, there is no need to use the variable sex.

## 2. What can we Ticket class tell us?
At first, we will look at the relation of Ticket class (\textit{Pclass}) and Passenger fare (\textit{fare}):
```{r}
train$Fare <- as.numeric(train$Fare)
train["Pclass.char"] <- as.character(train$Pclass)
ggplot(train, aes(Pclass.char, Fare)) + geom_boxplot()
```
Obviously, there is a significant positive correlation of the two variables, the upper class giving the most fare while the lower class giving the least fare. 

Keep that in mind, we continue with the difference in their sex: 

```{r}
mosaic(~ Sex + Pclass + Survived, data = train,  main = "Survival on Titanic",
       gp = shading_Friendly, legend= T )
```
The mosaic plots shows that a women of upper class has the highest chance of survival while a men the the Lower class the lowest chance of survival. Also, although the total number of male is three times the total number of female, but male has the lower probability of survival. In fact, when the disaster hit, women and children were the first priority  to go to the rescue  boat. 

## 3. Where did they embark?

```{r}
ggplot(train, aes(Survived, Pclass, colour = Embarked)) + geom_jitter() +
  labs(title = "Relation of Passerger class and Port of Embarktionin their survial chance")
```

The majority of passenger used  Southampton (S) to embark. However, only lower class (Pclass == 3) used Queenstown (\textit{Q}).

The two red dots are empty strings, that is why R could not detect any missing value. It turns out their personal information is different, but the others are the same. In fact, this cabin belongs to Mrs. George Nelson, and Miss Amelie is her maid. 
```{r}
train[which(train$Embarked == ""),]
```
Since these missing values are from cabin \textit{B28}, let see other variables in deck B:
```{r}
B <- train[grep("B", train$Cabin),]
CrossTable(B$Survived, B$Embarked, prop.c = T, prop.r = F, prop.t = F, prop.chisq = F)
```
In general, passenger with Cabin in deck B used Cherbourg (\textit{C}) and Southampton (\textit{S}) as their embarkation.  The percentage of survival of port Cherbourg (\textit{C}) is higher, therefore, we can impute the missing data above by \textit{C}.

```{r}
train$Embarked[train$Embarked == ""] <- "C"
train[c(62, 830),]
```


```{r}
table.embarked <- with(train, table(Survived, Embarked))
barplot(table.embarked, beside = T, legend= T, col = c("Lightblue", "blue"))
```

Only at port Cherbourge (\textit{C}), the percentage of survival is higher. 

## 5. Age:
```{r}
train[,"Age"] <- as.numeric(train[,"Age"])
cat("The number of missing value in variable Age: ",
   empty.string["Age"])

```



```{r}
age.missing <- train[is.na(train$Age), ]
ggplot(age.missing, aes(Title, Pclass.char)) + geom_jitter()
```




Those missing value are mainly are Mr. and Miss and Mrs, therefore, we can impute the missing data by mean of these group.  
```{r}
m <- mean(train[which(train$Title %in% c("Mr", "Mrs", "Miss")), "Age"])
m <- mean(train[, "Age"], na.rm =  T)
train$Age <- replace_na(train$Age, m)
```



```{r}
#age <- train[which(train$Age != ""),]
ggplot(train, aes(Pclass.char, Age, fill = Sex)) + geom_boxplot() + facet_grid(cols = vars(Survived))
```

Surprisingly, $50\%$ of survival male in middle class was less than 10 years old. Also, more than 50-year-old man is the least likely to survive through the disaster. This suggests a way to divide age into 3 smaller groups:
```{r}
train <- train %>% mutate(Age.char = case_when(Age < 15 ~ "young", 
                              Age < 50 ~ "middle.age",
                              Age < 100 ~ "old"))
ggplot(train, aes(Pclass, Age.char, colour = as.character(Survived))) + geom_jitter()
```

People in the first class has the highest chance to survive, especially when they are in middle age group. In contrast, middle-age men is the most likely died in the disaster. And in fact, the young iin seccond group will survive. 

## Family size:
At first, both variable \textit{SibSp} and \textit{Parch} contain information about family size, we can create a new variable as \textit{family.size} to obtain information about passenger's family
```{r}
train <- train %>% mutate(family.size = SibSp + Parch  +  1 )
table.sib <- with(train, table(Survived, family.size))
barplot(table.sib, beside = T, legend = T, col = c("Lightblue", "Blue"))

```
Surprisingly, the number of survival in family size from 2 to 4 is higher.

```{r}
ggplot(train, aes(as.character(family.size), log(Fare))) + geom_boxplot()
```
There is positive trend of $\ln(\text{Fare})$ and size of family, i.e, the large family size the more fare that ticket they paid. Possibly, it was because the Fare was given the same for all member in a family, not individually different.  Remember, when we discuss about missing value of Cabin B28, the two people there had the same fare value. Let's check this logic:

```{r}
fare <- train[which(train$family.size == 2), 
              c("Ticket", "Fare", "family.size", 
                "Pclass","Name", "Cabin")] %>% arrange(Ticket)
head(fare)
```
It seems that family members had the same ticket number would have the the same Fare. Now, we will write a function to check how correct this assumption is:


```{r}
train <- train %>% group_by(Ticket) %>% add_count() %>% mutate(mean.fare = mean(Fare))
nrow(train[which(train$Fare != train$mean.fare),])
```


As we expected, there are only 2 cases that does  not agree with our assumption. Therefore, it is actually a correlation between the family size and fare. To get rid of it, I will find the price that each person has to pay for their ticket and also fill 0 in price by the mean based on their class. 

```{r}
train <- train %>% mutate(price = Fare / n )
for (i in 1:3){
  m <- mean(train[which(train$Pclass == i & train$price != 0), "price"]$price)
  train[which(train$Pclass == i & train$price == 0), "price"] <- m
}

ggplot(train, aes(as.character(family.size), price)) + geom_boxplot()
```

Until this point, there is no dependence of ticket fare or price. But the variable "price" is actually dependent on \textit{Pclass}, and this happens in practice when you have to pay more to get the best service. 

# Variable Selection:

```{r, echo= FALSE}
library(SignifReg)
names(train)
train <- train %>% select(-c("PassengerId", "Name", "mean.fare", "Cabin", "n", "Ticket")) 
train <- train[,-1]
names(train)
null.model <- glm(Survived ~ 1, family = binomial, data = train)
full.model <- glm(Survived ~. , family = binomial, data = train)
scope = list(lower = formula(null.model), upper = formula(full.model))
var.sel <- step(null.model, scope = scope, scale = 0, trace = 0, direction = "both")
var.sel$formula
```
Forward selection suggest the model including \textit{Title, Pclass, family.size, Age, Fare, price, Embarked} as predictors. This model produced the AIC of 752.1, which is the same AIC as the smaller model with \textit{Title, Pclass, family.size, Age, Fare, price}. Therefore, I will drop \textit{Embarked} out of my model.

```{r}
keep <- c("Survived","Title", "Pclass", "family.size", "Age", "Fare", "price")
```


# Test Preparation:
The same way of data cleaning for test set:
```{r}
# Title feature:

test["Title"] <- str_split_fixed(test$Name, ", ", n = 2)[,2]
test["Title"] <- str_split_fixed(test$Title, ". ", n = 2)[, 1]
test["Title"] <- gsub("Major", "Officer", test[,"Title"], fixed = T)
test["Title"] <- gsub("Capt", "Officer", test[,"Title"], fixed = T)
test["Title"] <- gsub("Col", "Officer", test[,"Title"], fixed = T)
test["Title"] <- gsub("Rev", "Officer", test[,"Title"], fixed = T)
test["Title"] <- gsub("Don", "Officer", test[,"Title"], fixed = T)
test["Title"] <- gsub("Mme", "Miss", test[,"Title"], fixed = T)
test["Title"] <- gsub("Mlle", "Miss", test[,"Title"], fixed = T)
test["Title"] <- gsub("th", "Miss", test[,"Title"], fixed = T)
test["Title"] <- gsub("Jonkheer", "Mr", test[,"Title"], fixed = T)
test["Title"] <- gsub("Ms", "Miss", test[,"Title"], fixed = T)
test["Title"] <- gsub("Ms", "Miss", test[,"Title"], fixed = T)
test["Title"] <- gsub("Ms", "Mrs", test[,"Title"], fixed = T)
test["Title"] <- gsub("Lady", "Mrs", test[,"Title"], fixed = T)
test["Title"] <- gsub("Sir", "Mr", test[,"Title"], fixed = T)
unique(test$Title)
#This title is only in test set:
test["Title"] <- gsub("Officera", "Officer", test[,"Title"], fixed = T)
unique(test$Title)

test$Fare <- as.numeric(test$Fare)
test$Age <- as.numeric(test$Age)

#age.missing.test <- test[is.na(test$Age), ] 
#ggplot(age.missing.test, aes(Pclass, Sex)) + geom_jitter()
#m.test <- mean(test[which(test$Pclass == 3), "Age"], na.rm =  T)
test$Age <- replace_na(test$Age, mean(test$Age))

test <- test %>% mutate(family.size = SibSp + Parch  +  1 )
test <- test %>% group_by(Ticket) %>% add_count() %>% mutate(mean.fare = mean(Fare))

test <- test %>% mutate(price = Fare / n )
for (i in 1:3){
  m <- mean(test[which(test$Pclass == i & test$price != 0), "price"]$price)
  test[which(test$Pclass == i & test$price == 0), "price"] <- m
}


```

# Models:
```{r}
train.mod <- train[, keep]
keep.test <- keep[-1]
test.mod <- test[, keep.test]
```

## 1. Logistic Model:
```{r}



mod.reg <- glm(Survived ~., data = train.mod, family = binomial)
mod.fit <- as.numeric(fitted(mod.reg) >  0.4)
fit.tab <- xtabs(~ train.mod$Survived + mod.fit)
reg.acc.fit  <- (fit.tab[1,1] + fit.tab[2,2]) / sum(fit.tab)
summary(mod.reg)

```

## 2. Decision Tree:
- Decision Tree is a supervised learning method.
- Decision Tree is a graph to represent choices and their results in a form of a tree. 

```{r}
library(rpart)
library(rattle)
library(rpart.plot)
tree <- rpart(Survived ~., data = train.mod, method = "class")
fancyRpartPlot(tree)
```

```{r}
tree.fit <- predict(tree, train.mod, type = "class")
tree.tab.fit <- table(train.mod$Survived, tree.fit)
tree.acc.fit <- sum(diag(tree.tab.fit)) / sum(tree.tab.fit)

```


## 3. Random Forest:
- Random forest creates several random Decision Tree output is the aggregate of those trees. 

```{r}
library(randomForest)
set.seed(456)
fit.acc <- c()
x  <- c()
for (i in  seq(5, 50, by = 2)){
  rf <- randomForest(as.factor(Survived) ~., data = train.mod, ntree = i)
  conf.matrix <- rf$confusion
  fit.acc <- c(fit.acc,  sum(diag(conf.matrix)) / sum(conf.matrix))
  x <- c(x, i)
}

plot(x, fit.acc)
num.tree <- x[which.max(fit.acc)]
```





```{r}
library(randomForest)
rf <- randomForest(as.factor(Survived) ~., data = train.mod, ntree = num.tree)
rf.acc.fit <- sum(diag(rf$confusion)) / sum(rf$confusion)
```


## SVM:
- SVM (Support Vector Machine) is a supervised learning methods, which used to classified data. 
- Figure in wiki

```{r}
library(e1071)

svm.mod <- svm(as.factor(Survived) ~., data = train.mod, scale =  F)
svm.tab <- table(as.character(train.mod$Survived), svm.mod$fitted)
svm.fit.acc <- sum(diag(svm.tab)) / sum(svm.tab)

#make prediction:

```


## Comparsions:
```{r}
library(xtable)
library(kableExtra)

reg.pred <-  as.numeric(predict(mod.reg, test.mod) > 0.4)
reg.tab.pred <- xtabs(~ survive$Survived + reg.pred)
reg.acc.pred  <- (reg.tab.pred [1,1] + reg.tab.pred [2,2]) / sum(reg.tab.pred)


tree.pred <- predict(tree, test.mod, type = "class")
tree.tab.pred <- xtabs(~ survive$Survived + tree.pred)
tree.acc.pred  <- (tree.tab.pred [1,1] + tree.tab.pred [2,2]) / sum(tree.tab.pred)

rf.pred <- predict(rf, test.mod)
rf.tab.pred <- xtabs(~survive$Survived + rf.pred)
rf.acc.pred <- (rf.tab.pred [1,1] + rf.tab.pred [2,2]) / sum(rf.tab.pred)

#svm.pred <- predict(svm.mod, test.mod)
r <- data.frame(logistic = c(reg.acc.fit, reg.acc.pred), 
                tree = c(tree.acc.fit, tree.acc.pred),
                random.forest = c(rf.acc.fit, rf.acc.pred))

rownames(r) <- c("Fit accuarcy", "Prediction accuracy")
kable(r, align = "c", "latex", booktabs=T, escape = F) %>% 
  kable_styling(latex_options = "HOLD_position", position = "left") %>%
  row_spec(1, hline_after = T) %>%
  add_header_above(c(" ", "Summary results" = 2, ""), bold =  T, italic = T)

```


